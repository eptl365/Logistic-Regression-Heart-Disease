# ================================
# Logistic Regression (from scratch) on Heart Dataset
# ================================

# 0) Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --------------------------------------------------
# 1) Load / prepare data
# --------------------------------------------------
def load_data(path):
    """
    Loads the heart dataset from CSV.
    Splits into features (X) and labels (y).
    Standardizes features (mean=0, std=1).
    Returns: X (m,n), y (m,)
    """
    # Load dataset
    df = pd.read_csv(path)
    
    # Separate features and target
    X = df.drop(columns=["target"]).values   # shape: (m, n)
    y = df["target"].values                  # shape: (m,)
    
    # Standardize features (important for gradient descent stability)
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    return X, y

# Load dataset
X, y = load_data("heart.csv")
m, n = X.shape

# Add bias column (x0 = 1 for every example)
X = np.column_stack([np.ones(m), X])   # shape: (m, n+1)
n_with_bias = X.shape[1]


# --------------------------------------------------
# 2) Utility functions
# --------------------------------------------------
def sigmoid(z):
    """Sigmoid function σ(z) = 1 / (1 + e^-z)."""
    return 1 / (1 + np.exp(-z))

def predict_proba(X, w):
    """Compute predicted probabilities p = σ(Xw)."""
    return sigmoid(X @ w)

def binary_cross_entropy(y_true, y_prob, eps=1e-12):
    """
    Binary cross-entropy loss:
    L = - ( y log(p) + (1-y) log(1-p) )
    eps prevents log(0).
    """
    y_prob = np.clip(y_prob, eps, 1 - eps)
    return -np.mean(y_true * np.log(y_prob) + (1 - y_true) * np.log(1 - y_prob))

def gradient(X, y_true, y_prob):
    """
    Gradient of binary cross-entropy w.r.t weights:
    ∂J/∂w = (1/m) * X^T (y_prob - y_true)
    """
    return (1 / X.shape[0]) * (X.T @ (y_prob - y_true))


# --------------------------------------------------
# 3) Initialize parameters
# --------------------------------------------------
w = np.zeros(n_with_bias)   # weights (including bias)

# --------------------------------------------------
# 4) Hyperparameters
# --------------------------------------------------
alpha = 0.001          # learning rate
num_iter = 2000       # number of iterations

# For tracking
cost_history = []


# --------------------------------------------------
# 5) Gradient Descent loop
# --------------------------------------------------
for i in range(num_iter):
    # Forward pass: compute probabilities
    y_hat = predict_proba(X, w)
    
    # Compute loss (log-loss / cross-entropy)
    cost = binary_cross_entropy(y, y_hat)
    cost_history.append(cost)
    
    # Backward pass: compute gradient
    grad = gradient(X, y, y_hat)
    
    # Update weights
    w -= alpha * grad

# --------------------------------------------------
# 6) Final parameters
# --------------------------------------------------
print("Final parameters (w):")
print(w)   # w[0] is bias term
print("\nFinal cost:", cost_history[-1])

# --------------------------------------------------
# 7) Plot: Cost vs Iterations
# --------------------------------------------------
plt.figure()
plt.plot(range(len(cost_history)), cost_history, color="blue")
plt.xlabel("Iteration")
plt.ylabel("Cost (Log-Loss)")
plt.title("Training: Cost vs. Iterations")
plt.grid(True)
plt.show()


# --------------------------------------------------
# 8) Cost sensitivity plots (vs top 3 parameters)
# --------------------------------------------------
# Identify top-3 parameters by |w| (excluding bias)
param_indices = np.argsort(np.abs(w[1:]))[::-1][:3] + 1
print("Plotting cost sensitivity for parameter indices:", param_indices)

def compute_cost_given_w(mod_w):
    """Helper: compute cost for a modified parameter vector."""
    y_hat_mod = predict_proba(X, mod_w)
    return binary_cross_entropy(y, y_hat_mod)

# For each chosen parameter, sweep values and compute cost
for idx in param_indices:
    center = w[idx]
    sweep = np.linspace(center - 1.0, center + 1.0, 60)
    costs = []
    
    for val in sweep:
        w_tmp = w.copy()
        w_tmp[idx] = val
        costs.append(compute_cost_given_w(w_tmp))
    
    plt.figure()
    plt.plot(sweep, costs, color="red")
    plt.xlabel(f"Parameter w[{idx}]")
    plt.ylabel("Cost (Log-Loss)")
    plt.title(f"Cost vs Parameter w[{idx}] (holding others fixed)")
    plt.grid(True)
    plt.show()


# --------------------------------------------------
# 9) Inference helper
# --------------------------------------------------
def predict_label(X_new, w, threshold=0.5):
    """Return 0/1 predictions based on probability threshold."""
    return (predict_proba(X_new, w) >= threshold).astype(int)

# Example: training accuracy
preds = predict_label(X, w)
accuracy = (preds == y).mean()
print("Training accuracy:", accuracy)
